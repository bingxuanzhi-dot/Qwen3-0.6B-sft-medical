import os
import torch
# import swanlab
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq

import config
from data_helper import download_and_split_data, dataset_jsonl_transfer, load_and_process_data
from model_helper import load_model_tokenizer, predict

def main():
    print("ğŸš€ åˆå§‹åŒ–è®­ç»ƒæµç¨‹...")

    use_gpu = torch.cuda.is_available()
    
    # 1. åˆå§‹åŒ– SwanLab
    # swanlab.init(
    #     project=config.SWANLAB_PROJECT,
    #     mode="local",  # å¼€å¯ç¦»çº¿æ¨¡å¼ï¼Œæ—¥å¿—åªä¿å­˜åœ¨æœ¬åœ°ï¼Œä¸ä¸Šä¼ 
    #     config={
    #         "model_id": config.MODEL_ID,
    #         "device": "cuda" if use_gpu else "cpu"
    #     }
    # )


    # 2. å‡†å¤‡æ•°æ® (ä¸‹è½½ -> åˆ‡åˆ† -> æ ¼å¼åŒ–)
    download_and_split_data()
    
    if not os.path.exists(config.TRAIN_FORMAT_FILE):
        dataset_jsonl_transfer(config.TRAIN_FILE, config.TRAIN_FORMAT_FILE)
    if not os.path.exists(config.VAL_FORMAT_FILE):
        dataset_jsonl_transfer(config.VAL_FILE, config.VAL_FORMAT_FILE)

    # 3. åŠ è½½æ¨¡å‹ (ä¸‹è½½ -> åŠ è½½)
    model, tokenizer = load_model_tokenizer()

    # 4. å¤„ç†æ•°æ®é›†
    print("â³ æ­£åœ¨ Tokenize æ•°æ®é›†...")
    train_dataset, _ = load_and_process_data(config.TRAIN_FORMAT_FILE, tokenizer)
    eval_dataset, test_df = load_and_process_data(config.VAL_FORMAT_FILE, tokenizer)

    # 5. è®¾ç½®è®­ç»ƒå‚æ•°
    args = TrainingArguments(
        output_dir=config.OUTPUT_DIR,
        per_device_train_batch_size=config.BATCH_SIZE,
        per_device_eval_batch_size=config.BATCH_SIZE,
        gradient_accumulation_steps=config.GRAD_ACCUM_STEPS,
        eval_strategy="steps",
        logging_steps=config.LOGGING_STEPS,
        num_train_epochs=config.NUM_EPOCHS,
        save_steps=config.SAVE_STEPS,
        learning_rate=config.LEARNING_RATE,
        save_on_each_node=True,
        gradient_checkpointing=True if use_gpu else False, 
        report_to="none",
        run_name=config.SWANLAB_RUN_NAME,
        
        # =========== å…³é”®ï¼šCPU/GPU è‡ªåŠ¨åˆ‡æ¢ ===========
        fp16=(use_gpu and not torch.cuda.is_bf16_supported()), 
        bf16=(use_gpu and torch.cuda.is_bf16_supported()),
        use_cpu=(not use_gpu),
        dataloader_num_workers=4,
        # ============================================
    )

    # 6. åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    )

    # 7. å¼€å§‹è®­ç»ƒ
    print(f"ğŸ”¥ å¼€å§‹è®­ç»ƒ (æ¨¡å¼: {'GPU ğŸš€' if use_gpu else 'CPU ğŸ¢'})...")
    trainer.train()

    # 8. æµ‹è¯•
    print("ğŸ“ è®­ç»ƒå®Œæˆï¼Œç”Ÿæˆæµ‹è¯•ç»“æœ...")
    test_samples = test_df[:3]
    test_text_list = []

    for index, row in test_samples.iterrows():
        instruction = row['instruction']
        input_value = row['input']
        messages = [
            {"role": "system", "content": f"{instruction}"},
            {"role": "user", "content": f"{input_value}"}
        ]
        response = predict(messages, model, tokenizer)
        response_text = f"Question: {input_value}\n\nLLM Response:\n{response}"
        print("-" * 50)
        print(response_text)
        # test_text_list.append(swanlab.Text(response_text))

    # swanlab.log({"Prediction": test_text_list})
    # swanlab.finish()
    print("âœ… ä»»åŠ¡ç»“æŸï¼")

if __name__ == "__main__":
    main()
