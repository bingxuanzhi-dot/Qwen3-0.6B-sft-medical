import torch
import os
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from config import MAX_LENGTH, MODEL_ID, MODEL_PATH

def get_device_and_dtype():
    """
    è‡ªåŠ¨åˆ¤æ–­å½“å‰ç¯å¢ƒæ”¯æŒçš„è®¾å¤‡å’Œç²¾åº¦
    """
    if torch.cuda.is_available():
        device = "cuda"
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        device_map = "auto"
        print(f"âœ¨ æ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
    else:
        device = "cpu"
        dtype = torch.float32  # CPU å¿…é¡»ç”¨ float32ï¼Œå¦åˆ™æŠ¥é”™
        device_map = None      # CPU æ¨¡å¼ä¸‹ device_map è®¾ä¸º None
        print(f"ğŸ¢ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨è®¾å¤‡: {device}, ç²¾åº¦: {dtype}")
    
    return device, dtype, device_map

def load_model_tokenizer():
    """
    ä¸‹è½½(å¦‚ä¸å­˜åœ¨)å¹¶åŠ è½½æ¨¡å‹
    """
    # 1. å¦‚æœæœ¬åœ°æ²¡æœ‰æ¨¡å‹ï¼Œå…ˆä¸‹è½½
    if not os.path.exists(MODEL_PATH):
        print(f"â¬‡ï¸ æœ¬åœ°æœªæ‰¾åˆ°æ¨¡å‹ï¼Œæ­£åœ¨ä» ModelScope ä¸‹è½½: {MODEL_ID} ...")
        try:
            snapshot_download(MODEL_ID, cache_dir="./", local_dir=MODEL_PATH)
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼è¯·æ£€æŸ¥ '{MODEL_ID}' æ˜¯å¦å­˜åœ¨äº ModelScopeã€‚")
            raise e
    else:
        print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {MODEL_PATH}")

    # 2. è·å–è®¾å¤‡é…ç½®
    device, dtype, device_map = get_device_and_dtype()
    
    print("ğŸ“‚ æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, 
            use_fast=False, 
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH, 
            device_map=device_map, 
            torch_dtype=dtype,
            trust_remote_code=True
        )
        
        if dtype != torch.float32:
            model.enable_input_require_grads() 
            
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

def predict(messages, model, tokenizer):
    """
    æ¨ç†å‡½æ•°
    """
    device = model.device
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=MAX_LENGTH,
    )
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response
